# Logstash configuration for NovaCron log processing

input {
  # Beats input for log shipping
  beats {
    port => 5044
    host => "0.0.0.0"
  }
  
  # Syslog input
  syslog {
    port => 5514
    host => "0.0.0.0"
    type => "syslog"
  }
  
  # Docker logs via gelf
  gelf {
    port => 12201
    host => "0.0.0.0"
    type => "docker"
  }
  
  # HTTP input for application logs
  http {
    port => 8080
    host => "0.0.0.0"
    type => "application"
  }
  
  # Kafka input for high-volume logs
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["novacron-logs", "ml-pipeline-logs", "application-logs"]
    consumer_threads => 4
    decorate_events => true
  }
  
  # File input for log files
  file {
    path => "/var/log/novacron/*.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    type => "novacron-file"
  }
}

filter {
  # Common fields for all logs
  mutate {
    add_field => {
      "[@metadata][index_prefix]" => "novacron"
      "[@metadata][pipeline_version]" => "1.0"
    }
    add_tag => [ "novacron" ]
  }
  
  # Parse JSON logs
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
    }
    
    # Extract log level from structured logs
    if [level] {
      mutate {
        uppercase => [ "level" ]
        rename => { "level" => "log_level" }
      }
    }
  }
  
  # Process Docker container logs
  if [type] == "docker" {
    # Extract container information
    if [container_name] {
      mutate {
        add_field => {
          "service_name" => "%{container_name}"
          "environment" => "production"
        }
      }
    }
    
    # Parse application-specific logs
    if [container_name] =~ /^novacron-api/ {
      grok {
        match => { 
          "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} \[%{DATA:thread}\] %{DATA:logger} - %{GREEDYDATA:log_message}"
        }
        add_tag => [ "api", "application" ]
      }
    }
    
    if [container_name] =~ /^vm-manager/ {
      grok {
        match => { 
          "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} VM-%{DATA:vm_id}: %{GREEDYDATA:log_message}"
        }
        add_tag => [ "vm-manager", "application" ]
        add_field => { "component" => "vm-management" }
      }
    }
    
    if [container_name] =~ /^migration-service/ {
      grok {
        match => { 
          "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} Migration-%{DATA:migration_id}: %{GREEDYDATA:log_message}"
        }
        add_tag => [ "migration", "application" ]
        add_field => { "component" => "migration" }
      }
    }
    
    if [container_name] =~ /^ml-pipeline/ {
      grok {
        match => { 
          "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} \[%{DATA:stage}\] Model-%{DATA:model_name}: %{GREEDYDATA:log_message}"
        }
        add_tag => [ "ml-pipeline", "machine-learning" ]
        add_field => { "component" => "ml-pipeline" }
      }
    }
    
    if [container_name] =~ /^mle-star/ {
      grok {
        match => { 
          "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} Workflow-%{DATA:workflow_id} Stage-%{DATA:stage}: %{GREEDYDATA:log_message}"
        }
        add_tag => [ "mle-star", "machine-learning" ]
        add_field => { "component" => "mle-star" }
      }
    }
  }
  
  # Process application logs
  if [type] == "application" {
    # Parse HTTP access logs
    if [message] =~ /^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}/ {
      grok {
        match => { 
          "message" => "%{COMBINEDAPACHELOG}"
        }
        add_tag => [ "access_log" ]
      }
      
      # Convert response code to number
      mutate {
        convert => { 
          "response" => "integer" 
          "bytes" => "integer"
        }
      }
      
      # Categorize response codes
      if [response] >= 200 and [response] < 300 {
        mutate { add_field => { "response_category" => "success" } }
      } else if [response] >= 400 and [response] < 500 {
        mutate { add_field => { "response_category" => "client_error" } }
      } else if [response] >= 500 {
        mutate { add_field => { "response_category" => "server_error" } }
      }
    }
    
    # Parse error logs
    if [log_level] in ["ERROR", "FATAL"] {
      # Extract stack traces
      if [message] =~ /Exception|Error/ {
        grok {
          match => { 
            "message" => "(?<exception_type>[A-Za-z0-9_\.]+Exception): %{GREEDYDATA:exception_message}"
          }
          add_tag => [ "exception" ]
        }
      }
    }
  }
  
  # Process ML pipeline specific logs
  if "ml-pipeline" in [tags] or "mle-star" in [tags] {
    # Extract metrics from ML logs
    if [message] =~ /accuracy|precision|recall|f1_score|loss/ {
      grok {
        match => {
          "message" => "(?<metric_name>accuracy|precision|recall|f1_score|loss)[:=]\s*(?<metric_value>[0-9\.]+)"
        }
        add_tag => [ "ml_metrics" ]
      }
      
      if [metric_value] {
        mutate {
          convert => { "metric_value" => "float" }
        }
      }
    }
    
    # Extract training progress
    if [message] =~ /epoch|batch|step/ {
      grok {
        match => {
          "message" => "(?<progress_type>epoch|batch|step)[:=\s]+(?<progress_value>[0-9]+)"
        }
        add_tag => [ "training_progress" ]
      }
      
      if [progress_value] {
        mutate {
          convert => { "progress_value" => "integer" }
        }
      }
    }
  }
  
  # Parse timestamp if not already parsed
  if ![timestamp] and [@timestamp] {
    mutate {
      add_field => { "timestamp" => "%{@timestamp}" }
    }
  }
  
  # Convert log levels to consistent format
  if [log_level] {
    translate {
      field => "log_level"
      destination => "log_level_numeric"
      dictionary => {
        "TRACE" => "10"
        "DEBUG" => "20"
        "INFO" => "30"
        "WARN" => "40"
        "WARNING" => "40"
        "ERROR" => "50"
        "FATAL" => "60"
        "CRITICAL" => "60"
      }
      fallback => "30"
    }
    
    mutate {
      convert => { "log_level_numeric" => "integer" }
    }
  }
  
  # Add environment and cluster information
  mutate {
    add_field => {
      "cluster" => "novacron-production"
      "datacenter" => "us-west-2"
      "version" => "${NOVACRON_VERSION:unknown}"
    }
  }
  
  # GeoIP enrichment for external requests
  if [clientip] and [clientip] !~ /^(10\.|192\.168\.|172\.(1[6-9]|2[0-9]|3[01])\.)/ {
    geoip {
      source => "clientip"
      target => "geoip"
    }
  }
  
  # User agent parsing
  if [agent] {
    useragent {
      source => "agent"
      target => "user_agent"
    }
  }
  
  # Remove sensitive information
  mutate {
    remove_field => [ "password", "secret", "token", "api_key" ]
  }
  
  # Calculate processing time if start and end times are available
  if [request_start_time] and [request_end_time] {
    ruby {
      code => "
        start_time = Time.parse(event.get('request_start_time').to_s)
        end_time = Time.parse(event.get('request_end_time').to_s)
        event.set('request_duration_ms', ((end_time - start_time) * 1000).round(2))
      "
    }
  }
  
  # Index routing based on log type and age
  if "ml-pipeline" in [tags] or "mle-star" in [tags] {
    mutate {
      replace => { "[@metadata][index_prefix]" => "novacron-ml" }
    }
  } else if "access_log" in [tags] {
    mutate {
      replace => { "[@metadata][index_prefix]" => "novacron-access" }
    }
  } else if [log_level] in ["ERROR", "FATAL"] {
    mutate {
      replace => { "[@metadata][index_prefix]" => "novacron-errors" }
    }
  }
  
  # Hot-warm-cold architecture: set index suffix based on age
  ruby {
    code => "
      now = Time.now
      log_time = event.get('@timestamp')
      age_hours = (now - log_time) / 3600
      
      if age_hours < 24
        event.set('[@metadata][index_suffix]', 'hot')
      elsif age_hours < 168  # 7 days
        event.set('[@metadata][index_suffix]', 'warm')
      else
        event.set('[@metadata][index_suffix]', 'cold')
      end
    "
  }
}

output {
  # Main Elasticsearch output
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[@metadata][index_prefix]}-%{[@metadata][index_suffix]}-%{+YYYY.MM.dd}"
    template_name => "novacron"
    template => "/usr/share/logstash/templates/novacron-template.json"
    template_overwrite => true
    
    # Use document ID to prevent duplicates
    document_id => "%{[@metadata][fingerprint]}"
    
    # Bulk settings for performance
    flush_size => 1000
    idle_flush_time => 5
  }
  
  # Send critical errors to separate index for immediate alerting
  if [log_level] in ["FATAL", "CRITICAL"] or "exception" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "novacron-critical-%{+YYYY.MM.dd}"
      flush_size => 10
      idle_flush_time => 1
    }
  }
  
  # Send ML metrics to InfluxDB for time series analysis
  if "ml_metrics" in [tags] {
    influxdb {
      host => "influxdb"
      port => 8086
      database => "novacron_ml_metrics"
      measurement => "%{metric_name}"
      send_as_tags => ["model_name", "stage", "component"]
      data_points => {
        "value" => "%{metric_value}"
        "timestamp" => "%{@timestamp}"
      }
    }
  }
  
  # Send to Kafka for real-time processing
  kafka {
    topic_id => "processed-logs"
    bootstrap_servers => "kafka:9092"
    compression_type => "snappy"
  }
  
  # Debug output (comment out in production)
  # stdout {
  #   codec => rubydebug
  # }
}