{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Pipeline - novacron\n",
        "\n",
        "**MLE-Star Stage 2: Learning Pipeline Setup**\n",
        "\n",
        "This notebook implements the learning pipeline phase of the MLE-Star methodology:\n",
        "- Data preprocessing and feature engineering\n",
        "- Training and validation pipeline setup\n",
        "- Model training with monitoring\n",
        "- Training optimization and checkpointing\n",
        "\n",
        "**Framework:** {{framework}}\n",
        "\n",
        "**Date:** {{date}}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Training started at: {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Configuration and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Load project configuration\n",
        "config_path = Path('../configs/config.yaml')\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Load model design summary\n",
        "design_summary_path = Path('../outputs/reports/model_design_summary.yaml')\n",
        "if design_summary_path.exists():\n",
        "    with open(design_summary_path, 'r') as f:\n",
        "        design_summary = yaml.safe_load(f)\n",
        "    print(\"Model design summary loaded successfully!\")\n",
        "else:\n",
        "    print(\"Model design summary not found. Please run model design notebook first.\")\n",
        "    design_summary = {}\n",
        "\n",
        "print(f\"Experiment: {config['experiment']['name']}\")\n",
        "print(f\"Framework: {config['model']['framework']}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(config['data']['random_seed'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Preprocessing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Framework-specific imports\n",
        "framework = config['model']['framework']\n",
        "\n",
        "if framework == 'pytorch':\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "    from torch.optim.lr_scheduler import StepLR\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"PyTorch device: {device}\")\n",
        "    \n",
        "elif framework == 'tensorflow':\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers, callbacks\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    print(f\"TensorFlow GPU available: {tf.test.is_gpu_available()}\")\n",
        "    \n",
        "elif framework == 'scikit-learn':\n",
        "    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.metrics import classification_report, accuracy_score\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    print(\"Scikit-learn components imported\")\n",
        "\n",
        "# Common ML utilities\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Data loading function\n",
        "def load_and_preprocess_data():\n",
        "    \"\"\"Load and preprocess data according to configuration\"\"\"\n",
        "    \n",
        "    # For demonstration, create synthetic data similar to model design\n",
        "    n_samples = 5000\n",
        "    n_features = 10\n",
        "    \n",
        "    # Generate synthetic data\n",
        "    X = np.random.randn(n_samples, n_features)\n",
        "    # Create more complex relationship\n",
        "    y = ((X[:, 0] * 0.5 + X[:, 1] * 0.3 + X[:, 2] * 0.2 + \n",
        "          np.random.randn(n_samples) * 0.1) > 0).astype(int)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    feature_names = [f'feature_{i}' for i in range(n_features)]\n",
        "    df = pd.DataFrame(X, columns=feature_names)\n",
        "    df['target'] = y\n",
        "    \n",
        "    print(f\"Dataset loaded: {df.shape}\")\n",
        "    print(f\"Target distribution: {df['target'].value_counts().to_dict()}\")\n",
        "    \n",
        "    return df, feature_names\n",
        "\n",
        "# Load data\n",
        "df, feature_names = load_and_preprocess_data()\n",
        "\n",
        "# Prepare features and target\n",
        "X = df[feature_names].values\n",
        "y = df['target'].values\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Data splitting with stratification\n",
        "train_size = config['data']['train_split']\n",
        "val_size = config['data']['validation_split']\n",
        "test_size = config['data']['test_split']\n",
        "\n",
        "# First split: separate test set\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=test_size, random_state=config['data']['random_seed'],\n",
        "    stratify=y if config['data']['stratify'] else None\n",
        ")\n",
        "\n",
        "# Second split: separate train and validation\n",
        "val_ratio = val_size / (train_size + val_size)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=val_ratio, random_state=config['data']['random_seed'],\n",
        "    stratify=y_temp if config['data']['stratify'] else None\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"Validation set: {X_val.shape} ({len(X_val)/len(X)*100:.1f}%)\")\n",
        "print(f\"Test set: {X_test.shape} ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "\n",
        "# Check class distribution\n",
        "print(\"\\nClass distribution:\")\n",
        "print(f\"Train: {np.bincount(y_train)}\")\n",
        "print(f\"Validation: {np.bincount(y_val)}\")\n",
        "print(f\"Test: {np.bincount(y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Features scaled successfully!\")\n",
        "print(f\"Train features - Mean: {X_train_scaled.mean():.4f}, Std: {X_train_scaled.std():.4f}\")\n",
        "print(f\"Val features - Mean: {X_val_scaled.mean():.4f}, Std: {X_val_scaled.std():.4f}\")\n",
        "\n",
        "# Save scaler for later use\n",
        "scaler_path = Path('../outputs/models/feature_scaler.pkl')\n",
        "scaler_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(scaler_path, 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "print(f\"Scaler saved to: {scaler_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Architecture Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Define model architecture based on framework\n",
        "def create_model():\n",
        "    \"\"\"Create model based on framework configuration\"\"\"\n",
        "    \n",
        "    input_size = len(feature_names)\n",
        "    hidden_layers = config['model']['hidden_layers']\n",
        "    output_size = len(np.unique(y))\n",
        "    dropout_rate = config['model']['dropout_rate']\n",
        "    \n",
        "    if framework == 'pytorch':\n",
        "        class MLPClassifier(nn.Module):\n",
        "            def __init__(self, input_size, hidden_layers, output_size, dropout_rate):\n",
        "                super(MLPClassifier, self).__init__()\n",
        "                \n",
        "                layers = []\n",
        "                prev_size = input_size\n",
        "                \n",
        "                # Hidden layers\n",
        "                for hidden_size in hidden_layers:\n",
        "                    layers.extend([\n",
        "                        nn.Linear(prev_size, hidden_size),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(dropout_rate)\n",
        "                    ])\n",
        "                    prev_size = hidden_size\n",
        "                \n",
        "                # Output layer\n",
        "                layers.append(nn.Linear(prev_size, output_size))\n",
        "                \n",
        "                self.network = nn.Sequential(*layers)\n",
        "            \n",
        "            def forward(self, x):\n",
        "                return self.network(x)\n",
        "        \n",
        "        model = MLPClassifier(input_size, hidden_layers, output_size, dropout_rate)\n",
        "        model = model.to(device)\n",
        "        \n",
        "        print(f\"PyTorch model created:\")\n",
        "        print(f\"Input size: {input_size}\")\n",
        "        print(f\"Hidden layers: {hidden_layers}\")\n",
        "        print(f\"Output size: {output_size}\")\n",
        "        print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "        \n",
        "    elif framework == 'tensorflow':\n",
        "        model = keras.Sequential([\n",
        "            layers.Dense(hidden_layers[0], activation='relu', input_shape=(input_size,))\n",
        "        ])\n",
        "        \n",
        "        # Add hidden layers\n",
        "        for hidden_size in hidden_layers[1:]:\n",
        "            model.add(layers.Dense(hidden_size, activation='relu'))\n",
        "            model.add(layers.Dropout(dropout_rate))\n",
        "        \n",
        "        # Output layer\n",
        "        activation = 'sigmoid' if output_size == 2 else 'softmax'\n",
        "        output_units = 1 if output_size == 2 else output_size\n",
        "        model.add(layers.Dense(output_units, activation=activation))\n",
        "        \n",
        "        # Compile model\n",
        "        loss = 'binary_crossentropy' if output_size == 2 else 'sparse_categorical_crossentropy'\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=config['training']['learning_rate']),\n",
        "            loss=loss,\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        \n",
        "        print(\"TensorFlow model created:\")\n",
        "        model.summary()\n",
        "        \n",
        "    elif framework == 'scikit-learn':\n",
        "        # Create pipeline with preprocessing and model\n",
        "        model = Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('classifier', RandomForestClassifier(\n",
        "                n_estimators=100,\n",
        "                random_state=config['data']['random_seed'],\n",
        "                n_jobs=-1\n",
        "            ))\n",
        "        ])\n",
        "        \n",
        "        print(\"Scikit-learn pipeline created:\")\n",
        "        print(f\"Components: {[name for name, _ in model.steps]}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create model\n",
        "model = create_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Pipeline Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Training configuration\n",
        "training_config = config['training']\n",
        "batch_size = training_config['batch_size']\n",
        "epochs = training_config['epochs']\n",
        "learning_rate = training_config['learning_rate']\n",
        "\n",
        "print(f\"Training Configuration:\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Epochs: {epochs}\")\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "\n",
        "# Initialize training history\n",
        "training_history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': [],\n",
        "    'epoch': [],\n",
        "    'lr': []\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Framework-specific training implementation\n",
        "def train_model():\n",
        "    \"\"\"Train model based on framework\"\"\"\n",
        "    \n",
        "    if framework == 'pytorch':\n",
        "        return train_pytorch_model()\n",
        "    elif framework == 'tensorflow':\n",
        "        return train_tensorflow_model()\n",
        "    elif framework == 'scikit-learn':\n",
        "        return train_sklearn_model()\n",
        "\n",
        "def train_pytorch_model():\n",
        "    \"\"\"PyTorch training implementation\"\"\"\n",
        "    \n",
        "    # Prepare data loaders\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.FloatTensor(X_train_scaled),\n",
        "        torch.LongTensor(y_train)\n",
        "    )\n",
        "    val_dataset = TensorDataset(\n",
        "        torch.FloatTensor(X_val_scaled),\n",
        "        torch.LongTensor(y_val)\n",
        "    )\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    # Setup optimizer and loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "    \n",
        "    # Training loop\n",
        "    model.train()\n",
        "    best_val_acc = 0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        \n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += batch_y.size(0)\n",
        "            train_correct += predicted.eq(batch_y).sum().item()\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += batch_y.size(0)\n",
        "                val_correct += predicted.eq(batch_y).sum().item()\n",
        "        \n",
        "        # Calculate metrics\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "        train_loss_avg = train_loss / len(train_loader)\n",
        "        val_loss_avg = val_loss / len(val_loader)\n",
        "        \n",
        "        # Update history\n",
        "        training_history['epoch'].append(epoch + 1)\n",
        "        training_history['train_loss'].append(train_loss_avg)\n",
        "        training_history['train_acc'].append(train_acc)\n",
        "        training_history['val_loss'].append(val_loss_avg)\n",
        "        training_history['val_acc'].append(val_acc)\n",
        "        training_history['lr'].append(scheduler.get_last_lr()[0])\n",
        "        \n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), '../outputs/models/best_model.pth')\n",
        "        \n",
        "        # Print progress\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}: '\n",
        "                  f'Train Loss: {train_loss_avg:.4f}, Train Acc: {train_acc:.2f}%, '\n",
        "                  f'Val Loss: {val_loss_avg:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "        \n",
        "        scheduler.step()\n",
        "        model.train()\n",
        "    \n",
        "    return model, training_history\n",
        "\n",
        "def train_tensorflow_model():\n",
        "    \"\"\"TensorFlow training implementation\"\"\"\n",
        "    \n",
        "    # Callbacks\n",
        "    callback_list = [\n",
        "        callbacks.ModelCheckpoint(\n",
        "            '../outputs/models/best_model.h5',\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=10,\n",
        "            verbose=1\n",
        "        ),\n",
        "        callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=20,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        X_train_scaled, y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(X_val_scaled, y_val),\n",
        "        callbacks=callback_list,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    # Update training history\n",
        "    for epoch in range(len(history.history['loss'])):\n",
        "        training_history['epoch'].append(epoch + 1)\n",
        "        training_history['train_loss'].append(history.history['loss'][epoch])\n",
        "        training_history['train_acc'].append(history.history['accuracy'][epoch] * 100)\n",
        "        training_history['val_loss'].append(history.history['val_loss'][epoch])\n",
        "        training_history['val_acc'].append(history.history['val_accuracy'][epoch] * 100)\n",
        "        training_history['lr'].append(learning_rate)  # Simplified\n",
        "    \n",
        "    return model, training_history\n",
        "\n",
        "def train_sklearn_model():\n",
        "    \"\"\"Scikit-learn training implementation\"\"\"\n",
        "    \n",
        "    # Train model\n",
        "    print(\"Training scikit-learn model...\")\n",
        "    model.fit(X_train, y_train)  # Pipeline handles scaling\n",
        "    \n",
        "    # Evaluate on train and validation sets\n",
        "    train_pred = model.predict(X_train)\n",
        "    val_pred = model.predict(X_val)\n",
        "    \n",
        "    train_acc = accuracy_score(y_train, train_pred) * 100\n",
        "    val_acc = accuracy_score(y_val, val_pred) * 100\n",
        "    \n",
        "    # Update training history (simplified for sklearn)\n",
        "    training_history['epoch'] = [1]\n",
        "    training_history['train_acc'] = [train_acc]\n",
        "    training_history['val_acc'] = [val_acc]\n",
        "    training_history['train_loss'] = [0]  # Not applicable\n",
        "    training_history['val_loss'] = [0]    # Not applicable\n",
        "    training_history['lr'] = [0]          # Not applicable\n",
        "    \n",
        "    print(f\"Training completed!\")\n",
        "    print(f\"Train Accuracy: {train_acc:.2f}%\")\n",
        "    print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
        "    \n",
        "    # Save model\n",
        "    with open('../outputs/models/best_model.pkl', 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    \n",
        "    return model, training_history\n",
        "\n",
        "# Start training\n",
        "print(f\"Starting {framework} model training...\")\n",
        "trained_model, history = train_model()\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Results Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Plot training history\n",
        "if len(history['epoch']) > 1:  # Only plot if we have multiple epochs\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Training and validation loss\n",
        "    axes[0, 0].plot(history['epoch'], history['train_loss'], label='Train Loss', marker='o')\n",
        "    axes[0, 0].plot(history['epoch'], history['val_loss'], label='Val Loss', marker='s')\n",
        "    axes[0, 0].set_title('Training and Validation Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "    \n",
        "    # Training and validation accuracy\n",
        "    axes[0, 1].plot(history['epoch'], history['train_acc'], label='Train Acc', marker='o')\n",
        "    axes[0, 1].plot(history['epoch'], history['val_acc'], label='Val Acc', marker='s')\n",
        "    axes[0, 1].set_title('Training and Validation Accuracy')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "    \n",
        "    # Learning rate schedule\n",
        "    axes[1, 0].plot(history['epoch'], history['lr'], marker='o')\n",
        "    axes[1, 0].set_title('Learning Rate Schedule')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Learning Rate')\n",
        "    axes[1, 0].set_yscale('log')\n",
        "    axes[1, 0].grid(True)\n",
        "    \n",
        "    # Overfitting analysis\n",
        "    overfitting = [train - val for train, val in zip(history['train_acc'], history['val_acc'])]\n",
        "    axes[1, 1].plot(history['epoch'], overfitting, marker='o', color='red')\n",
        "    axes[1, 1].set_title('Overfitting Analysis (Train - Val Accuracy)')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Accuracy Difference (%)')\n",
        "    axes[1, 1].axhline(y=0, color='black', linestyle='--')\n",
        "    axes[1, 1].grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../outputs/figures/training_history.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Training history visualization skipped (single epoch or sklearn model)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Training summary statistics\n",
        "print(\"Training Summary:\")\n",
        "print(f\"Framework: {framework}\")\n",
        "print(f\"Total epochs: {len(history['epoch'])}\")\n",
        "\n",
        "if len(history['train_acc']) > 0:\n",
        "    final_train_acc = history['train_acc'][-1]\n",
        "    final_val_acc = history['val_acc'][-1]\n",
        "    best_val_acc = max(history['val_acc'])\n",
        "    \n",
        "    print(f\"Final train accuracy: {final_train_acc:.2f}%\")\n",
        "    print(f\"Final validation accuracy: {final_val_acc:.2f}%\")\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "    print(f\"Overfitting (train - val): {final_train_acc - final_val_acc:.2f}%\")\n",
        "    \n",
        "    # Find best epoch\n",
        "    best_epoch = history['val_acc'].index(best_val_acc) + 1\n",
        "    print(f\"Best epoch: {best_epoch}\")\n",
        "    \n",
        "    if len(history['train_loss']) > 1 and any(loss > 0 for loss in history['train_loss']):\n",
        "        final_train_loss = history['train_loss'][-1]\n",
        "        final_val_loss = history['val_loss'][-1]\n",
        "        print(f\"Final train loss: {final_train_loss:.4f}\")\n",
        "        print(f\"Final validation loss: {final_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Evaluation on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Load best model and evaluate on test set\n",
        "def evaluate_test_set():\n",
        "    \"\"\"Evaluate trained model on test set\"\"\"\n",
        "    \n",
        "    if framework == 'pytorch':\n",
        "        # Load best model\n",
        "        model.load_state_dict(torch.load('../outputs/models/best_model.pth'))\n",
        "        model.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
        "            outputs = model(test_tensor)\n",
        "            _, predicted = outputs.max(1)\n",
        "            test_predictions = predicted.cpu().numpy()\n",
        "    \n",
        "    elif framework == 'tensorflow':\n",
        "        # Load best model\n",
        "        best_model = keras.models.load_model('../outputs/models/best_model.h5')\n",
        "        test_predictions = (best_model.predict(X_test_scaled) > 0.5).astype(int).flatten()\n",
        "    \n",
        "    elif framework == 'scikit-learn':\n",
        "        # Use trained model\n",
        "        test_predictions = trained_model.predict(X_test)\n",
        "    \n",
        "    return test_predictions\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"Evaluating model on test set...\")\n",
        "test_pred = evaluate_test_set()\n",
        "\n",
        "# Calculate test metrics\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "test_acc = accuracy_score(y_test, test_pred)\n",
        "print(f\"\\nTest Set Results:\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, test_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.savefig('../outputs/figures/test_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Training Pipeline Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create comprehensive training report\n",
        "training_report = {\n",
        "    'experiment_name': config['experiment']['name'],\n",
        "    'framework': framework,\n",
        "    'training_config': training_config,\n",
        "    'model_architecture': {\n",
        "        'input_size': len(feature_names),\n",
        "        'hidden_layers': config['model']['hidden_layers'],\n",
        "        'output_size': len(np.unique(y)),\n",
        "        'dropout_rate': config['model']['dropout_rate']\n",
        "    },\n",
        "    'data_info': {\n",
        "        'total_samples': len(X),\n",
        "        'train_samples': len(X_train),\n",
        "        'val_samples': len(X_val),\n",
        "        'test_samples': len(X_test),\n",
        "        'n_features': len(feature_names)\n",
        "    },\n",
        "    'training_results': {\n",
        "        'epochs_trained': len(history['epoch']),\n",
        "        'final_train_accuracy': history['train_acc'][-1] if history['train_acc'] else 0,\n",
        "        'final_val_accuracy': history['val_acc'][-1] if history['val_acc'] else 0,\n",
        "        'best_val_accuracy': max(history['val_acc']) if history['val_acc'] else 0,\n",
        "        'test_accuracy': test_acc * 100,\n",
        "        'overfitting_score': (history['train_acc'][-1] - history['val_acc'][-1]) if history['train_acc'] and history['val_acc'] else 0\n",
        "    },\n",
        "    'artifacts_created': [\n",
        "        'outputs/models/best_model.*',\n",
        "        'outputs/models/feature_scaler.pkl',\n",
        "        'outputs/figures/training_history.png',\n",
        "        'outputs/figures/test_confusion_matrix.png',\n",
        "        'outputs/reports/training_pipeline_report.yaml'\n",
        "    ],\n",
        "    'next_steps': [\n",
        "        'Model evaluation with detailed metrics',\n",
        "        'Hyperparameter tuning',\n",
        "        'Model analysis and interpretation',\n",
        "        'Systematic testing implementation'\n",
        "    ],\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "# Save training report\n",
        "report_path = Path('../outputs/reports/training_pipeline_report.yaml')\n",
        "report_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(report_path, 'w') as f:\n",
        "    yaml.dump(training_report, f, default_flow_style=False)\n",
        "\n",
        "print(f\"Training pipeline report saved to: {report_path}\")\n",
        "\n",
        "# Save training history\n",
        "history_path = Path('../outputs/models/training_history.pkl')\n",
        "with open(history_path, 'wb') as f:\n",
        "    pickle.dump(history, f)\n",
        "\n",
        "print(f\"Training history saved to: {history_path}\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING PIPELINE COMPLETED SUCCESSFULLY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Framework: {framework}\")\n",
        "print(f\"Best Validation Accuracy: {training_report['training_results']['best_val_accuracy']:.2f}%\")\n",
        "print(f\"Test Accuracy: {training_report['training_results']['test_accuracy']:.2f}%\")\n",
        "print(f\"Model saved and ready for evaluation stage\")\n",
        "print(\"\\nNext: Run notebook 03_model_evaluation.ipynb\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}