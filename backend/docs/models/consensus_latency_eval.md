# Consensus Latency LSTM Autoencoder Evaluation Report

**Generated:** 2025-11-14 10:01:52

## Model Architecture

- **Type:** LSTM Autoencoder with Attention Mechanism
- **Encoder:** 3-layer LSTM (128 → 64 → 32 units) + Attention
- **Decoder:** 3-layer LSTM (32 → 64 → 128 units)
- **Sequence Length:** 30 timesteps
- **Features:** 12 consensus metrics

### Features

1. `queue_depth` - Consensus queue depth
2. `proposals_pending` - Pending proposals count
3. `proposals_committed` - Committed proposals count
4. `latency_p50` - 50th percentile latency (ms)
5. `latency_p95` - 95th percentile latency (ms)
6. `latency_p99` - 99th percentile latency (ms)
7. `leader_changes` - Leadership change frequency
8. `quorum_size` - Quorum size
9. `active_nodes` - Active node count
10. `network_tier` - Network type (LAN/WAN)
11. `dwcp_mode` - DWCP operating mode
12. `consensus_type` - Consensus algorithm type

## Performance Metrics

### Detection Accuracy: **97.46%**

| Metric | Value |
|--------|-------|
| **Precision** | 1.0000 |
| **Recall** | 0.9492 |
| **F1 Score** | 0.9739 |
| **Detection Accuracy** | 0.9746 |
| **ROC AUC** | 0.9960 |

### Confusion Matrix

|                | Predicted Normal | Predicted Anomaly |
|----------------|------------------|-------------------|
| **Actual Normal** | 1597 | 0 |
| **Actual Anomaly** | 19 | 355 |

### Threshold Configuration

- **Anomaly Threshold:** 0.638022
- **Method:** Optimized for F1 score
- **True Anomalies:** 374 / 1971
- **Predicted Anomalies:** 355 / 1971

## Target Achievement

**Target:** ≥98% Detection Accuracy (Precision + Recall) / 2

**Status:** ✗ NOT ACHIEVED

**Gap:** 0.54% below target

## Anomaly Detection Strategy

The model uses **reconstruction error-based anomaly detection**:

1. Train LSTM autoencoder on normal consensus behavior
2. Calculate reconstruction error (MSE) for each sequence
3. Sequences with error > threshold are flagged as anomalies
4. Detects high-latency episodes caused by:
   - Network congestion
   - Leader election storms
   - Queue overflow
   - Byzantine attacks

## Model Deployment

### CLI Command for Training

```bash
python train_lstm_autoencoder.py \
  --sequence-length 30 \
  --epochs 100 \
  --batch-size 64 \
  --encoding-dim 16 \
  --output models/consensus
```

### Inference Example

```python
import joblib
import numpy as np
from tensorflow import keras

# Load model and artifacts
model = keras.models.load_model('models/consensus/consensus_latency_autoencoder.keras')
scaler = joblib.load('models/consensus/consensus_scaler.pkl')

# Load metadata for threshold
import json
with open('models/consensus/consensus_metadata.json') as f:
    metadata = json.load(f)
threshold = metadata['anomaly_threshold']

# Prepare sequence (30 timesteps × 12 features)
sequence = np.array([...])  # Shape: (30, 12)
sequence_scaled = scaler.transform(sequence.reshape(-1, 12)).reshape(1, 30, 12)

# Predict
reconstruction = model.predict(sequence_scaled)
error = np.mean(np.square(sequence_scaled - reconstruction))

# Detect anomaly
is_anomaly = error > threshold
print(f"Reconstruction Error: {error:.6f}")
print(f"Threshold: {threshold:.6f}")
print(f"Anomaly Detected: {is_anomaly}")
```

## Visualizations

See attached files:
- `evaluation_report.png` - Comprehensive evaluation plots
- `training_curves.png` - Training and validation loss curves

## Recommendations

1. **Production Deployment:** Model ready for production use
2. **Monitoring:** Track reconstruction errors in real-time
3. **Retraining:** Retrain monthly with new consensus data
4. **Alerting:** Configure alerts for sequences exceeding threshold
5. **Feature Engineering:** Consider adding network RTT metrics

---

*Report generated by LSTM Autoencoder Training Pipeline v2.0*
