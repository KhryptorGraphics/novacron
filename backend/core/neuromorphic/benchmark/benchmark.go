package benchmark

import (
	"context"
	"fmt"
	"math"
	"sync"
	"time"

	"github.com/khryptorgraphics/novacron/backend/core/neuromorphic/metrics"
	"github.com/khryptorgraphics/novacron/backend/core/neuromorphic/snn"
)

// BenchmarkSuite runs comprehensive neuromorphic benchmarks
type BenchmarkSuite struct {
	mu              sync.RWMutex
	metricsCollector *metrics.MetricsCollector
	results         []*BenchmarkResult
}

// BenchmarkResult represents benchmark results
type BenchmarkResult struct {
	Name            string    `json:"name"`
	Duration        float64   `json:"duration_ms"`
	Latency         float64   `json:"latency_us"`
	Throughput      float64   `json:"throughput_inferences_per_sec"`
	PowerUsage      float64   `json:"power_usage_mw"`
	EnergyPerInf    float64   `json:"energy_per_inference_mj"`
	Accuracy        float64   `json:"accuracy"`
	NeuronCount     int64     `json:"neuron_count"`
	SynapseCount    int64     `json:"synapse_count"`
	Timestamp       time.Time `json:"timestamp"`
	ComparisonToGPU float64   `json:"comparison_to_gpu_x"`
	ComparisonToCNN float64   `json:"comparison_to_cnn_x"`
}

// NewBenchmarkSuite creates a new benchmark suite
func NewBenchmarkSuite() *BenchmarkSuite {
	return &BenchmarkSuite{
		metricsCollector: metrics.NewMetricsCollector(),
		results:          make([]*BenchmarkResult, 0),
	}
}

// BenchmarkInferenceLatency benchmarks inference latency
func (bs *BenchmarkSuite) BenchmarkInferenceLatency(ctx context.Context, network *snn.SNNNetwork, iterations int) (*BenchmarkResult, error) {
	latencies := make([]float64, 0, iterations)
	totalPower := 0.0

	for i := 0; i < iterations; i++ {
		start := time.Now()

		// Run one inference
		inputSpikes := generateRandomSpikes(100, 10)
		_, err := network.Step(ctx, inputSpikes)
		if err != nil {
			return nil, err
		}

		elapsed := time.Since(start).Microseconds()
		latencies = append(latencies, float64(elapsed))

		// Simulate power measurement
		powerMw := 50.0 + math.Sin(float64(i))*10.0
		totalPower += powerMw

		select {
		case <-ctx.Done():
			return nil, ctx.Err()
		default:
		}
	}

	// Calculate statistics
	avgLatency, minLatency, maxLatency := calculateStats(latencies)
	avgPower := totalPower / float64(iterations)

	result := &BenchmarkResult{
		Name:         "Inference Latency",
		Duration:     avgLatency / 1000.0,
		Latency:      avgLatency,
		Throughput:   1000000.0 / avgLatency, // inferences per second
		PowerUsage:   avgPower,
		EnergyPerInf: (avgPower * avgLatency) / 1000.0, // mJ
		Timestamp:    time.Now(),
	}

	result.ComparisonToGPU = 10000.0 / avgLatency  // GPU: ~10ms
	result.ComparisonToCNN = 50000.0 / avgLatency  // CNN: ~50ms

	bs.mu.Lock()
	bs.results = append(bs.results, result)
	bs.mu.Unlock()

	fmt.Printf("âœ… Latency Benchmark: Avg=%.2fÂµs, Min=%.2fÂµs, Max=%.2fÂµs\n",
		avgLatency, minLatency, maxLatency)

	return result, nil
}

// BenchmarkEnergyEfficiency benchmarks energy efficiency
func (bs *BenchmarkSuite) BenchmarkEnergyEfficiency(ctx context.Context, network *snn.SNNNetwork, duration time.Duration) (*BenchmarkResult, error) {
	startTime := time.Now()
	totalEnergy := 0.0
	inferences := 0

	for time.Since(startTime) < duration {
		inferenceStart := time.Now()

		// Run inference
		inputSpikes := generateRandomSpikes(100, 10)
		_, err := network.Step(ctx, inputSpikes)
		if err != nil {
			return nil, err
		}

		latency := time.Since(inferenceStart).Microseconds()
		powerMw := 50.0 + math.Sin(float64(inferences))*10.0
		energyMj := (powerMw * float64(latency)) / 1000.0

		totalEnergy += energyMj
		inferences++

		select {
		case <-ctx.Done():
			return nil, ctx.Err()
		default:
		}
	}

	elapsed := time.Since(startTime).Seconds()
	avgPower := totalEnergy / elapsed / 1000.0 // Average power in mW
	energyPerInf := totalEnergy / float64(inferences)
	inferencesPerJoule := 1000.0 / energyPerInf

	result := &BenchmarkResult{
		Name:         "Energy Efficiency",
		Duration:     elapsed * 1000,
		PowerUsage:   avgPower,
		EnergyPerInf: energyPerInf,
		Throughput:   float64(inferences) / elapsed,
		Timestamp:    time.Now(),
	}

	// GPU comparison: 200W, 1000 inf/sec = 200mJ/inf
	gpuEnergyPerInf := 200.0
	result.ComparisonToGPU = gpuEnergyPerInf / energyPerInf

	// CNN comparison: 50mJ/inf
	cnnEnergyPerInf := 50.0
	result.ComparisonToCNN = cnnEnergyPerInf / energyPerInf

	bs.mu.Lock()
	bs.results = append(bs.results, result)
	bs.mu.Unlock()

	fmt.Printf("âœ… Energy Efficiency: %.3f mJ/inf, %.0f inf/J, %.0fx better than GPU\n",
		energyPerInf, inferencesPerJoule, result.ComparisonToGPU)

	return result, nil
}

// BenchmarkAccuracy benchmarks accuracy
func (bs *BenchmarkSuite) BenchmarkAccuracy(ctx context.Context, network *snn.SNNNetwork, testData [][]float64, labels []int) (*BenchmarkResult, error) {
	correct := 0
	total := len(testData)

	for i, input := range testData {
		// Convert input to spikes
		inputSpikes := encodeToSpikes(input)

		// Run inference
		outputSpikes, err := network.Run(ctx, 100.0, func(t float64) []snn.Spike {
			if t < 1.0 {
				return inputSpikes
			}
			return nil
		})

		if err != nil {
			return nil, err
		}

		// Decode output
		predicted := decodeSpikes(outputSpikes)

		if predicted == labels[i] {
			correct++
		}

		select {
		case <-ctx.Done():
			return nil, ctx.Err()
		default:
		}
	}

	accuracy := float64(correct) / float64(total)

	result := &BenchmarkResult{
		Name:      "Accuracy",
		Accuracy:  accuracy,
		Timestamp: time.Now(),
	}

	bs.mu.Lock()
	bs.results = append(bs.results, result)
	bs.mu.Unlock()

	fmt.Printf("âœ… Accuracy Benchmark: %.2f%% (%d/%d correct)\n",
		accuracy*100, correct, total)

	return result, nil
}

// BenchmarkThroughput benchmarks throughput
func (bs *BenchmarkSuite) BenchmarkThroughput(ctx context.Context, network *snn.SNNNetwork, duration time.Duration) (*BenchmarkResult, error) {
	startTime := time.Now()
	inferences := 0

	for time.Since(startTime) < duration {
		inputSpikes := generateRandomSpikes(100, 10)
		_, err := network.Step(ctx, inputSpikes)
		if err != nil {
			return nil, err
		}

		inferences++

		select {
		case <-ctx.Done():
			return nil, ctx.Err()
		default:
		}
	}

	elapsed := time.Since(startTime).Seconds()
	throughput := float64(inferences) / elapsed

	result := &BenchmarkResult{
		Name:       "Throughput",
		Duration:   elapsed * 1000,
		Throughput: throughput,
		Timestamp:  time.Now(),
	}

	// GPU: ~1000 inf/sec at 200W
	gpuThroughput := 1000.0
	result.ComparisonToGPU = throughput / gpuThroughput

	bs.mu.Lock()
	bs.results = append(bs.results, result)
	bs.mu.Unlock()

	fmt.Printf("âœ… Throughput Benchmark: %.0f inferences/sec (%.1fx vs GPU baseline)\n",
		throughput, result.ComparisonToGPU)

	return result, nil
}

// BenchmarkScalability benchmarks scalability with different network sizes
func (bs *BenchmarkSuite) BenchmarkScalability(ctx context.Context) ([]*BenchmarkResult, error) {
	results := make([]*BenchmarkResult, 0)
	neuronCounts := []int64{1000, 10000, 100000, 1000000}

	for _, neuronCount := range neuronCounts {
		// Create network
		stdpConfig := &snn.STDPConfig{Enable: false}
		network := snn.NewSNNNetwork(1.0, stdpConfig)

		// Add neurons
		for i := int64(0); i < neuronCount; i++ {
			network.AddNeuron(snn.LIF)
		}

		// Benchmark
		start := time.Now()
		inputSpikes := generateRandomSpikes(int(neuronCount/10), 10)
		_, err := network.Step(ctx, inputSpikes)
		if err != nil {
			return nil, err
		}
		latency := time.Since(start).Microseconds()

		result := &BenchmarkResult{
			Name:         fmt.Sprintf("Scalability_%d_neurons", neuronCount),
			NeuronCount:  neuronCount,
			Latency:      float64(latency),
			Timestamp:    time.Now(),
		}

		results = append(results, result)

		fmt.Printf("âœ… Scalability with %d neurons: %.2f Âµs\n", neuronCount, float64(latency))
	}

	bs.mu.Lock()
	bs.results = append(bs.results, results...)
	bs.mu.Unlock()

	return results, nil
}

// RunAllBenchmarks runs all benchmarks
func (bs *BenchmarkSuite) RunAllBenchmarks(ctx context.Context, network *snn.SNNNetwork) error {
	fmt.Println("ðŸš€ Running Neuromorphic Computing Benchmarks...")

	// Latency benchmark
	_, err := bs.BenchmarkInferenceLatency(ctx, network, 1000)
	if err != nil {
		return err
	}

	// Energy efficiency benchmark
	_, err = bs.BenchmarkEnergyEfficiency(ctx, network, 5*time.Second)
	if err != nil {
		return err
	}

	// Throughput benchmark
	_, err = bs.BenchmarkThroughput(ctx, network, 5*time.Second)
	if err != nil {
		return err
	}

	// Scalability benchmark
	_, err = bs.BenchmarkScalability(ctx)
	if err != nil {
		return err
	}

	fmt.Println("âœ… All benchmarks completed successfully!")
	return nil
}

// GetResults returns all benchmark results
func (bs *BenchmarkSuite) GetResults() []*BenchmarkResult {
	bs.mu.RLock()
	defer bs.mu.RUnlock()
	return bs.results
}

// Helper functions
func generateRandomSpikes(count, maxTime int) []snn.Spike {
	spikes := make([]snn.Spike, count)
	for i := 0; i < count; i++ {
		spikes[i] = snn.Spike{
			NeuronID:  int64(i),
			Timestamp: math.Mod(float64(i)*3.7, float64(maxTime)),
			Weight:    1.0,
		}
	}
	return spikes
}

func encodeToSpikes(input []float64) []snn.Spike {
	spikes := make([]snn.Spike, 0)
	for i, val := range input {
		if val > 0.5 {
			spikes = append(spikes, snn.Spike{
				NeuronID:  int64(i),
				Timestamp: 0.0,
				Weight:    val,
			})
		}
	}
	return spikes
}

func decodeSpikes(spikes []snn.Spike) int {
	if len(spikes) == 0 {
		return 0
	}
	return int(spikes[0].NeuronID % 10)
}

func calculateStats(values []float64) (avg, min, max float64) {
	if len(values) == 0 {
		return 0, 0, 0
	}

	sum := 0.0
	min = values[0]
	max = values[0]

	for _, v := range values {
		sum += v
		if v < min {
			min = v
		}
		if v > max {
			max = v
		}
	}

	avg = sum / float64(len(values))
	return avg, min, max
}
