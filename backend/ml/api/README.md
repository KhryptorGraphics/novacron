# Compression Selector API

REST API for the Compression Selector ML model, designed for integration with the DWCP protocol.

## Quick Start

### 1. Install Dependencies

```bash
pip install flask flask-cors
```

### 2. Start Server

```bash
python3 compression_api.py
```

Server runs on `http://localhost:5000`

### 3. Test API

```bash
./test_api.sh
```

## API Endpoints

### Health Check

**GET** `/health`

Check if API and model are ready.

```bash
curl http://localhost:5000/health
```

Response:
```json
{
  "status": "healthy",
  "model_loaded": true
}
```

---

### Predict Compression Algorithm

**POST** `/api/v1/compression/predict`

Get optimal compression algorithm for given parameters.

Request:
```json
{
  "data_type": "text",
  "size": 1048576,
  "latency": 100,
  "bandwidth": 100
}
```

Response:
```json
{
  "algorithm": "zstd",
  "confidence": 0.9802,
  "all_probabilities": {
    "zstd": 0.9802,
    "lz4": 0.0157,
    "snappy": 0.0041,
    "none": 0.0000
  },
  "timestamp": "2025-11-14T03:45:00Z"
}
```

Example:
```bash
curl -X POST http://localhost:5000/api/v1/compression/predict \
  -H "Content-Type: application/json" \
  -d '{
    "data_type": "text",
    "size": 1048576,
    "latency": 100,
    "bandwidth": 100
  }'
```

---

### Batch Predictions

**POST** `/api/v1/compression/batch`

Get predictions for multiple requests in one call.

Request:
```json
{
  "requests": [
    {"data_type": "text", "size": 1048576, "latency": 100, "bandwidth": 100},
    {"data_type": "json", "size": 10240, "latency": 10, "bandwidth": 1000}
  ]
}
```

Response:
```json
{
  "predictions": [
    {"algorithm": "zstd", "confidence": 0.98},
    {"algorithm": "none", "confidence": 0.66}
  ],
  "total": 2
}
```

---

### List Algorithms

**GET** `/api/v1/compression/algorithms`

Get information about supported compression algorithms.

Response:
```json
{
  "algorithms": [
    {
      "name": "zstd",
      "description": "High compression ratio, medium speed",
      "use_case": "Large files, slow networks"
    },
    ...
  ]
}
```

---

### Model Information

**GET** `/api/v1/model/info`

Get model metadata and performance metrics.

Response:
```json
{
  "model_type": "Random Forest Classifier",
  "accuracy": 0.9965,
  "features": [...],
  "output_classes": ["zstd", "lz4", "snappy", "none"],
  "feature_importance": [...]
}
```

## Integration with Go/DWCP

### HTTP Client Example

```go
package main

import (
    "bytes"
    "encoding/json"
    "net/http"
)

type PredictionRequest struct {
    DataType  string  `json:"data_type"`
    Size      int64   `json:"size"`
    Latency   float64 `json:"latency"`
    Bandwidth float64 `json:"bandwidth"`
}

type PredictionResponse struct {
    Algorithm  string  `json:"algorithm"`
    Confidence float64 `json:"confidence"`
}

func SelectCompression(dataType string, size int64, latency, bandwidth float64) (string, error) {
    req := PredictionRequest{
        DataType:  dataType,
        Size:      size,
        Latency:   latency,
        Bandwidth: bandwidth,
    }

    body, _ := json.Marshal(req)

    resp, err := http.Post(
        "http://localhost:5000/api/v1/compression/predict",
        "application/json",
        bytes.NewBuffer(body),
    )
    if err != nil {
        return "none", err
    }
    defer resp.Body.Close()

    var result PredictionResponse
    json.NewDecoder(resp.Body).Decode(&result)

    return result.Algorithm, nil
}
```

## Parameters

### data_type
Type of data being transmitted.

**Options**: `text`, `binary`, `structured`, `json`, `protobuf`

### size
Data size in bytes.

**Range**: 1 to 1,000,000,000+ (1GB+)

### latency
Maximum acceptable latency in milliseconds.

**Range**: 1 to 10,000+ (1ms to 10s)

### bandwidth
Available bandwidth in Mbps.

**Range**: 1 to 10,000+ (1Mbps to 10Gbps)

## Response Algorithms

### zstd
- **Compression Ratio**: High
- **Speed**: Medium
- **CPU Usage**: High
- **Best For**: Large files, slow networks

### lz4
- **Compression Ratio**: Low
- **Speed**: Very High
- **CPU Usage**: Low
- **Best For**: Real-time, tight latency

### snappy
- **Compression Ratio**: Medium
- **Speed**: High
- **CPU Usage**: Medium
- **Best For**: General purpose

### none
- **Compression Ratio**: None
- **Speed**: Instant
- **CPU Usage**: None
- **Best For**: Fast networks, very tight latency

## Error Handling

### 400 Bad Request
Invalid input parameters.

```json
{
  "error": "Missing required field: data_type"
}
```

### 503 Service Unavailable
Model not loaded.

```json
{
  "error": "Model not loaded"
}
```

### 500 Internal Server Error
Server error.

```json
{
  "error": "Internal server error"
}
```

## Performance

- **Model Load Time**: ~50ms (one-time on startup)
- **Prediction Latency**: ~0.6ms per request
- **Memory Usage**: ~2MB
- **Throughput**: ~1,600 requests/second

## Production Deployment

### Using Gunicorn

```bash
pip install gunicorn
gunicorn -w 4 -b 0.0.0.0:5000 compression_api:app
```

### Using Docker

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 5000

CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:5000", "compression_api:app"]
```

## Monitoring

Add logging and metrics:

```python
from prometheus_client import Counter, Histogram

prediction_counter = Counter('predictions_total', 'Total predictions')
prediction_latency = Histogram('prediction_latency_seconds', 'Prediction latency')

@prediction_latency.time()
def predict_compression():
    prediction_counter.inc()
    # ... prediction logic
```

---

**Version**: 1.0.0
**Last Updated**: 2025-11-14
**Status**: Production Ready âœ“
